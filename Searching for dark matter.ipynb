{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport xgboost as xg\nimport tables as tb\nfrom tqdm import tqdm\nfrom itertools import cycle, islice\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.datasets import make_blobs\nfrom IPython import display\nfrom sklearn.neighbors import BallTree, KDTree, DistanceMetric\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import Normalizer\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom keras.layers.core import Dense, Activation\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nimport utils\n\n%matplotlib inline\n%pylab inline\n\n# Make the images larger\nplt.rcParams['figure.figsize'] = (16, 9)\nfigsize = (10,10)\npoint_size=150\npoint_border=0.8\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis kernel accompanies the [blog post](https://lavanya.ai/searching-for-dark-matter/) I wrote about searching for traces of dark matter in data produced by CERN.\n\nIâ€™ve recently been having a lot of fun playing with the Large Hadron Collider datasets, and I thought Iâ€™ll share some of the things Iâ€™ve learnt along the way. The 2 main things Iâ€™ll be exploring in this post are:\n\n- **Particle identification**: training a classifier to detect electrons, protons, muons, kaons and pions\n- **Searching for dark matter**: training a classifier to distinguish between background noise and the signal, and then applying clustering algorithms to find potential traces of dark matter in this signal\n\nYou can get more background by reading the blog post here: https://lavanya.ai/searching-for-dark-matter/\n\nPS: I would love to hear what you think about the post and the kernel, and any suggestions you have for how I can improve it! This kernel is a WIP. I'll add to it over the next few weeks as I explore the dataset further. Thank you Coursera for presenting some of the ideas that inspired this kernel!"},{"metadata":{},"cell_type":"markdown","source":"# Particle Identification"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"training = pd.read_csv('../input/particle-identification/training.csv')\ntest = pd.read_csv('../input/particle-identification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the dataset description, we know that the features describe particle responses in the detector systems, and represent the following:\n\n- ID - id value for tracks (presents only in the test file for the submitting purposes)\n- Label - string valued observable denoting particle types. Can take values \"Electron\", \"Muon\", \"Kaon\", \"Proton\", \"Pion\" and \"Ghost\". This column is absent in the test file.\n- FlagSpd - flag (0 or 1), if reconstructed track passes through Spd\n- FlagPrs - flag (0 or 1), if reconstructed track passes through Prs\n- FlagBrem - flag (0 or 1), if reconstructed track passes through Brem\n- FlagEcal - flag (0 or 1), if reconstructed track passes through Ecal\n- FlagHcal - flag (0 or 1), if reconstructed track passes through Hcal\n- FlagRICH1 - flag (0 or 1), if reconstructed track passes through the first RICH detector\n- FlagRICH2 - flag (0 or 1), if reconstructed track passes through the second RICH detector\n- FlagMuon - flag (0 or 1), if reconstructed track passes through muon stations (Muon)\n- SpdE - energy deposit associated to the track in the Spd\n- PrsE - energy deposit associated to the track in the Prs\n- EcalE - energy deposit associated to the track in the Hcal\n- HcalE - energy deposit associated to the track in the Hcal\n- PrsDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Prs\n- BremDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Brem\n- TrackP - particle momentum\n- TrackPt - particle transverse momentum\n- TrackNDoFSubdetector1  - number of degrees of freedom for track fit using hits in the tracking sub-detector1\n- TrackQualitySubdetector1 - chi2 quality of the track fit using hits in the tracking sub-detector1\n- TrackNDoFSubdetector2 - number of degrees of freedom for track fit using hits in the tracking sub-detector2\n- TrackQualitySubdetector2 - chi2 quality of the track fit using hits in the  tracking sub-detector2\n- TrackNDoF - number of degrees of freedom for track fit using hits in all tracking sub-detectors\n- TrackQualityPerNDoF - chi2 quality of the track fit per degree of freedom\n- TrackDistanceToZ - distance between track and z-axis (beam axis)\n- Calo2dFitQuality - quality of the 2d fit of the clusters in the calorimeter \n- Calo3dFitQuality - quality of the 3d fit in the calorimeter with assumption that particle was electron\n- EcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Ecal\n- EcalDLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from Ecal\n- EcalShowerLongitudinalParameter - longitudinal parameter of Ecal shower\n- HcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Hcal\n- HcalDLLbeMuon - delta log-likelihood for a particle candidate to be using information from Hcal\n- RICHpFlagElectron - flag (0 or 1) if momentum is greater than threshold for electrons to produce Cherenkov light\n- RICHpFlagProton - flag (0 or 1) if momentum is greater than threshold for protons to produce Cherenkov light\n- RICHpFlagPion - flag (0 or 1) if momentum is greater than threshold for pions to produce Cherenkov light\n- RICHpFlagKaon - flag (0 or 1) if momentum is greater than threshold for kaons to produce Cherenkov light\n- RICHpFlagMuon - flag (0 or 1) if momentum is greater than threshold for muons to produce Cherenkov light\n- RICH_DLLbeBCK  - delta log-likelihood for a particle candidate to be background using information from RICH\n- RICH_DLLbeKaon - delta log-likelihood for a particle candidate to be kaon using information from RICH\n- RICH_DLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from RICH\n- RICH_DLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from RICH\n- RICH_DLLbeProton - delta log-likelihood for a particle candidate to be proton using information from RICH\n- MuonFlag - muon flag (is this track muon) which is determined from muon stations\n- MuonLooseFlag muon flag (is this track muon) which is determined from muon stations using looser criteria\n- MuonLLbeBCK - log-likelihood for a particle candidate to be not muon using information from muon stations\n- MuonLLbeMuon - log-likelihood for a particle candidate to be muon using information from muon stations\n- DLLelectron - delta log-likelihood for a particle candidate to be electron using information from all subdetectors\n- DLLmuon - delta log-likelihood for a particle candidate to be muon using information from all subdetectors\n- DLLkaon - delta log-likelihood for a particle candidate to be kaon using information from all subdetectors\n- DLLproton - delta log-likelihood for a particle candidate to be proton using information from all subdetectors\n- GhostProbability - probability for a particle candidate to be ghost track. This variable is an output of classification model used in the tracking algorithm.\n\nSpd stands for Scintillating Pad Detector, Prs - Preshower, Ecal - electromagnetic calorimeter, Hcal - hadronic calorimeter, Brem denotes traces of the particles that were deflected by detector."},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_percentile(arr, w, q):\n    left = 0.\n    right = (w).sum()\n    sort_inds = np.argsort(arr, axis=0)\n    if left/right >= q/100.:\n        return arr[0]\n    for i in sort_inds:\n        left += w[i]\n        if left/right >= q/100.:\n            return arr[i]\n\ndef plot(prediction, spectator, cut, percentile=True, weights=None, n_bins=100,\n              color='b', marker='o', ms=4, label=\"MVA\", fmt='o', markeredgecolor='b', markeredgewidth=2, ecolor='b'):\n    if weights is None:\n        weights = np.ones(len(prediction))\n\n    if percentile:\n        if weights is None:\n            cut = np.percentile(prediction, 100-cut)\n        else:\n            cut = my_percentile(prediction, weights, 100-cut)\n    \n    edges = np.linspace(spectator.min(), spectator.max(), n_bins)\n    \n    xx = []\n    yy = []\n    xx_err = []\n    yy_err = []\n    \n    for i_edge in range(len(edges)-1):\n\n        left = edges[i_edge]\n        right = edges[i_edge + 1]\n        \n        N_tot_bin = weights[((spectator >= left) * (spectator < right))].sum()\n        N_cut_bin = weights[((spectator >= left) * (spectator < right) * (prediction >= cut))].sum()\n        \n        if N_tot_bin != 0:\n            \n            x = 0.5 * (right + left)\n            y = 1. * N_cut_bin / N_tot_bin\n            \n            if y > 1.:\n                y = 1.\n            if y < 0:\n                y = 0\n            \n            xx.append(x)\n            yy.append(y)\n            \n            x_err = 0.5 * (right - left)\n            y_err = np.sqrt(y*(1-y)/N_tot_bin)\n            \n            xx_err.append(x_err)\n            yy_err.append(y_err)\n        \n        else:\n            pass\n\n    plt.errorbar(xx, yy, yerr=yy_err, xerr=xx_err, fmt=fmt, color=color, marker=marker, ms=ms, label=label, markeredgecolor=markeredgecolor, markeredgewidth=markeredgewidth, ecolor=ecolor)\n    \n    return cut","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"training['Label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train an AdaBoost model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get numeric labels for each of the string labels, to make them compatible with our model\nlabel_class_correspondence = {'Electron': 0, 'Ghost': 1, 'Kaon': 2, 'Muon': 3, 'Pion': 4, 'Proton': 5}\nclass_label_correspondence = {0: 'Electron', 1: 'Ghost', 2: 'Kaon', 3: 'Muon', 4: 'Pion', 5: 'Proton'}\n\ndef get_class_ids(labels):\n    return np.array([label_class_correspondence[alabel] for alabel in labels])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training['Class'] = get_class_ids(training.Label.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we're trying to predict the label (or class), we remove them from our list of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(set(training.columns) - {'Label', 'Class'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the training data into training and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid = train_test_split(training, random_state=42, train_size=0.90, test_size=0.10)\nprint(train.shape[0], valid.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, random_state=42,\n                             base_estimator=DecisionTreeClassifier(max_depth=6, min_samples_leaf=30, random_state=42))\nclf.fit(train[features].values, train.Class.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba_clf = clf.predict_proba(valid[features].values)\nlog_loss(valid.Class.values, proba_clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def roc_curves(predictions, labels):\n    plt.figure(figsize=(9, 6))\n    u_labels = np.unique(labels)\n    for lab in u_labels:\n        y_true = labels == lab\n        y_pred = predictions[:, lab]\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        auc = roc_auc_score(y_true, y_pred)\n        plt.plot(tpr, 1-fpr, linewidth=3, label=class_label_correspondence[lab] + ', AUC = ' + str(np.round(auc, 4)))\n        plt.xlabel('Signal efficiency (TPR)', size=15)\n        plt.ylabel(\"Background rejection (1 - FPR)\", size=15)\n        plt.xticks(size=15)\n        plt.yticks(size=15)\n        plt.xlim(0., 1)\n        plt.ylim(0., 1)\n        plt.legend(loc='lower left', fontsize=15)\n        plt.title('One particle vs rest ROC curves', loc='right', size=15)\n        plt.grid(b=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_curves(proba_clf, valid.Class.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def efficiency(predictions, labels, spectator, eff=60, n_bins=20, xlabel='Spectator'):\n    plt.figure(figsize=(5.5*2, 3.5*3))\n    u_labels = np.unique(labels)\n    for lab in u_labels:\n        y_true = labels == lab\n        pred = predictions[y_true, lab]\n        spec = spectator[y_true]\n        plt.subplot(3, 2, lab+1)\n        plot(pred, spec, cut=eff, percentile=True, weights=None, n_bins=n_bins, color='1', marker='o', \n                  ms=7, label=class_label_correspondence[lab], fmt='o')\n        \n        plt.plot([spec.min(), spec.max()], [eff / 100., eff / 100.], label='Global signal efficiecny', color='r', linewidth=3)\n        plt.legend(loc='best', fontsize=12)\n        plt.xticks(size=12)\n        plt.yticks(size=12)\n        plt.ylabel('Signal efficiency (TPR)', size=12)\n        plt.xlabel(xlabel,size=12)\n        plt.ylim(0, 1)\n        plt.xlim(spec.min(), spec.max())\n        plt.grid(b=1)\n    plt.tight_layout()\n        \n\ndef efficiency_on_p(predictions, labels, spectator, eff=60, n_bins=20):\n    sel = spectator < 200 * 10**3\n    efficiency(predictions[sel], labels[sel], spectator[sel] / 10**3, eff, n_bins, 'Momentum, GeV/c')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"efficiency_on_p(proba_clf, valid.Class.values, valid.TrackP.values, 60, 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def efficiency_on_pt(predictions, labels, spectator, eff=60, n_bins=20):\n    sel = spectator < 10 * 10**3\n    efficiency(predictions[sel], labels[sel], spectator[sel] / 10**3, eff, n_bins, 'Transverse momentum, GeV/c')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"efficiency_on_pt(proba_clf, valid.Class.values, valid.TrackPt.values, 60, 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train a Neural Network"},{"metadata":{},"cell_type":"markdown","source":"Training a second, neural network model and comparing performance via ROC curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"def nn_model(input_dim):\n    model = Sequential()\n    model.add(Dense(100, input_dim=input_dim))\n    model.add(Activation('tanh'))\n\n    model.add(Dense(6))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn = nn_model(len(features))\nnn.fit(train[features].values, np_utils.to_categorical(train.Class.values), verbose=1, nb_epoch=5, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba_nn = nn.predict_proba(valid[features].values)\nlog_loss(valid.Class.values, proba_nn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba_nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_curves(proba_nn, valid.Class.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just to compare performances, here are the ROC curves for our AdaBoostClassifier model."},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_curves(proba_clf, valid.Class.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, AdaBoostClassifier performs slightly better than the neural net across the board for all particles."},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# release unreferenced memory to ensure we don't run out of memory\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, test, valid, training","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Searching for dark matter"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_hdf('../input/dark-matter-from-opera-experiments/open30.h5') # pick just a single brick\ntest = pd.read_hdf('../input/dark-matter-from-opera-experiments/test.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.reset_index(drop=True)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.signal.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a dataframe of signals for clustering later on"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_signal = train.copy()\ntrain_signal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_signal = train_signal[train['signal']==1]\ntrain_signal.signal.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_signal.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore dataset and find neighboring base tracks"},{"metadata":{},"cell_type":"markdown","source":"Plot showers in the brick"},{"metadata":{"trusted":true},"cell_type":"code","source":"CMAP = sns.diverging_palette(220, 20, s=99, as_cmap=True, n=2500)\n\ndef plot3D(X, target, elev=0, azim=0, title=None, sub=111):\n    x = X[:, 0]\n    y = X[:, 1]\n    z = X[:, 2]\n    \n    fig = plt.figure(figsize=(12, 8))\n    ax = Axes3D(fig)\n    mappab = ax.scatter(x, y, z, c=target, cmap=CMAP)\n\n    if title is not None:\n        ax.set_title(title)\n    ax.set_xlabel('Component 1')\n    ax.set_ylabel('Component 2')\n    ax.set_zlabel('Component 3')\n\n    ax.view_init(elev=elev, azim=azim)\n    fig.colorbar(mappable=mappab, label='Target variable')\n    plt.show()\n    \nfeat_XY = ['TX', 'TY', 'X', 'Y']\nfirst = train.loc[train.data_ind == 21, :]\nplot3D(first.loc[first.signal==1, ['Z', 'X', 'Y']].values,\n       first.loc[first.signal==1].signal.values, elev=20, azim=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot3D(first.loc[first.signal==1, ['Z', 'X', 'Y']].values,\n       first.loc[first.signal==1].signal.values, elev=45, azim=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Closer look at base track distribution along the axes"},{"metadata":{"trusted":true},"cell_type":"code","source":"axis = 'X'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"axis = 'Y'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"axis = 'Z'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group base tracks from neighboring plates (see blog post for intuition behind why we do this)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_neighbours(df, k, metric='minkowski'):\n    res = []\n    \n    for data_ind in tqdm(np.unique(df.data_ind)):\n        ind = df.loc[df.data_ind == data_ind].copy()\n        ind[['TX', 'TY']] *= 1293\n        values = np.unique(ind.Z)\n        \n        for j in range(1, len(values)):\n            z, z_next = (ind.loc[ind.Z == values[j-1]].copy(),\n                         ind.loc[ind.Z == values[j]].copy())\n            \n            b_tree = BallTree(z_next[feat_XY], metric=metric)\n            d, i = b_tree.query(z[feat_XY], k=min(k, len(z_next)))\n            \n            for m in range(i.shape[1]):\n                data = z_next.iloc[i[:, m]]\n                z_copy = z.copy()\n                for col in feat_XY + ['Z']:\n                    z_copy[col + '_pair'] = data[col].values\n                res.append(z_copy)\n            \n        res.append(z_next)\n        \n    res = pd.concat(res)\n    for col in feat_XY + ['Z']:\n        res['d' + col] = res[col].values - res[col + '_pair'].values\n    return res\n\ndef balance_train(df, k):\n    data = add_neighbours(df, k=k)\n    noise = data.event_id == -999\n    signal, not_signal = data.loc[np.logical_not(noise)], data.loc[noise]\n    noise_part = not_signal.sample(len(signal))\n    return pd.concat([signal, noise_part]).reset_index(drop=True)\ntrain = []\nfor file in glob.glob('../input/dark-matter-from-opera-experiments/open*.h5')[:5]: # just 5 bricks\n    train.append(balance_train(pd.read_hdf(file), k=3))\ntrain = pd.concat(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using a Neural Net to separate signal from background"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train.signal\nX_train = train.drop(['event_id', 'signal', 'data_ind'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = Normalizer()\nX_train_norm = transformer.fit_transform(X_train.fillna(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\ntrain_index, val_index = next(sss.split(X_train_norm, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nn_model(input_dim):\n    model = Sequential()\n    model.add(Dense(256, input_dim=input_dim))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(128))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(64))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer=Adam())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto'),\n            ModelCheckpoint('{val_loss:.4f}.hdf5', monitor='val_loss', verbose=2, save_best_only=True, mode='auto')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn = nn_model(X_train_norm.shape[1])\nnn.fit(X_train_norm, y_train, validation_split=0.2, epochs=20, verbose=2, batch_size=256, shuffle=True, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepared_test = add_neighbours(test, k=3)\nX_test = prepared_test.drop(['data_ind'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_norm = transformer.transform(X_test.fillna(0))\nX_test = transformer.transform(X_test.fillna(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_norm[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probas = nn.predict_proba(X_test_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probas = np.squeeze(probas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'id': prepared_test.index, 'signal': probas}).groupby('id')\nagg = df.aggregate(('mean')).loc[:, ['signal']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using an XGBoost classifier to separate signal from background"},{"metadata":{},"cell_type":"markdown","source":"### Note: If Kernel fails below it's because it has run out of memory while training the XGBoost. If so, please download the notebook and run it on another machine."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n        'n_estimators':[10, 20], \n        'max_depth':[15],\n}\n\nclass XGBClassifier_tmp(XGBClassifier):\n    def predict(self, X):\n        return XGBClassifier.predict_proba(self, X)[:, 1]\n\n'''\nclf = GridSearchCV(XGBClassifier_tmp(learning_rate=0.05, subsample=0.8,\n                                     colsample_bytree=0.8, n_jobs=20), \n                   param_grid=param_grid, n_jobs=3,\n                   scoring='roc_auc',\n                   cv=StratifiedKFold(3, shuffle=True, random_state=0),\n                   verbose=7)\nclf.fit(X_train, y_train)\nxgb_class = clf.best_estimator_\n'''\n\nxgb = XGBClassifier_tmp(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n         colsample_bytree=0.8, gamma=0, learning_rate=0.05,\n         max_delta_step=0, max_depth=15, min_child_weight=1, missing=None,\n         n_estimators=100, nthread=None,\n         objective='binary:logistic', random_state=0, reg_alpha=0,\n         reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n         subsample=0.8, n_jobs=24)\n\nprepared_test = add_neighbours(test, k=3)\nX_test = prepared_test.drop(['data_ind'], axis=1)\nxgb.fit(X_train, y_train)\nprobas = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probas = clf.predict(X_test)\nprobas.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'id': prepared_test.index, 'signal': probas}).groupby('id')\nagg = df.aggregate(('mean')).loc[:, ['signal']]\nagg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using KMeans to find signal clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_signal.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\neps=0.000001\nmin_samples=2\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\nclustering_labels = dbscan.fit_predict(train_signal)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find the optimal number of clusters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(train_signal)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\nk = [inertias.index(i) for i in inertias]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(k, inertias, linewidth=2.0)\nline, = plt.plot(k, inertias, 'o')\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Inertia\", fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The inertia's rate of decline flattens around k=6 clusters. So we'll train a KMeans with 6 clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=6, random_state=42).fit(train_signal)\nclustering_labels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_signal.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clustering_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = train_signal\nclusters['cluster'] = clustering_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sample = train_signal.sample(frac=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_sample.X, X_sample.Y, X_sample.Z, c=X_sample.cluster)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}